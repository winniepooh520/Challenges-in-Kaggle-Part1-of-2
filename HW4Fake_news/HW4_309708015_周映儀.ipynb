{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW4 - Fake News Detection\n",
    "本次作業將針對假新聞作分析，預測一則新聞是否reliable (繳交期限：12/10 23:55)\n",
    "1: fake\n",
    "0: true\n",
    "請分別利用GBDT、LightGBM、xgboost對\"train.csv\"的資料建模，並用\"test.csv\"進行測試\n",
    "註：\"test.csv\"的label在\"sample_submission.csv\"裡面\n",
    "(資料來源：https://www.kaggle.com/c/fakenewskdd2020/data)\n",
    "\n",
    "作業流程：\n",
    "\n",
    "1. 資料前處理\n",
    " a. 讀取\"train.csv\"與\"test.csv\"並利用分割符號切割、建立train&test之DataFrame\n",
    "註：分割符號為tab(\\t)\n",
    "\n",
    " b. 去除停頓詞stop words \n",
    "可參考：\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "自訂stop words\n",
    "https://stackoverflow.com/questions/52712254/how-to-eliminate-stop-words-only-using-scikit-learn\n",
    "\n",
    " c. 文字探勘前處理，將文字轉換成向量，像是常見的方法 tf-idf、word2vec...等\n",
    "\n",
    "可參考：\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text\n",
    "\n",
    "Word2vec\n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "2. 建模：分別使用以下三種模型\n",
    "xgboost\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_intro.html#install-xgboost\n",
    "\n",
    "GBDT\n",
    "\n",
    "LightGBM\n",
    "https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/\n",
    "\n",
    "3. 評估模型\n",
    "利用\"test.csv\"的資料對2.所建立的模型進行測試，並計算Accuracy、Precision、Recall、F-measure\n",
    "\n",
    "請將以下檔案壓縮成 HW4_學號_姓名.zip：\n",
    "\n",
    "1. 討論報告PDF (含截圖跟簡短說明、討論等等)，需提及：\n",
    "文字探勘前處理的方法\n",
    "GBDT、LightGBM、xgboost 模型之結果比較\n",
    "\n",
    "2. Python程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Get the latest from TODAY Sign up for our news...     1\n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...     1\n",
       "2  It’s safe to say that Instagram Stories has fa...     0\n",
       "3  Much like a certain Amazon goddess with a lass...     0\n",
       "4  At a time when the perfect outfit is just one ...     0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"train.csv\", sep='\\t', encoding='utf-8')\n",
    "test = pd.read_csv(\"test.csv\", sep='\\t', encoding='utf-8')\n",
    "test_label = pd.read_csv(\"sample_submission.csv\", sep='\\t', encoding='utf-8')\n",
    "train.head()\n",
    "\n",
    "# type(train) : pandas.core.frame.DataFrame\n",
    "# train.columns : Index(['text', 'label'], dtype='object')\n",
    "# print( len(train['text'])): 4987\n",
    "# print(len(test['text'])): 1247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1247 entries, 0 to 1246\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id,label  1247 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 9.9+ KB\n"
     ]
    }
   ],
   "source": [
    "test_label.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>The 2017 Teen Choice Awards ceremony was held ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>The concert, part of “The Joshua Tree Tour,” w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Selena Gomez refuses to talk to her mother abo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>This is worse than a lump of coal in your stoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Luann De Lesseps is going to rehab after her a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>1244</td>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1245</td>\n",
       "      <td>Jaden Smith claims that the Four Seasons Hotel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>1246</td>\n",
       "      <td>Overview (3)  Mini Bio (1)  Faith Hill was bor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>1247</td>\n",
       "      <td>CLOSE Aaron Paul dishes on 'The Path'  Aaron P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1248</td>\n",
       "      <td>Meghan Edmonds was showered with love at her b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1247 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  label\n",
       "0        2  The 2017 Teen Choice Awards ceremony was held ...      1\n",
       "1        3  The concert, part of “The Joshua Tree Tour,” w...      1\n",
       "2        4  Selena Gomez refuses to talk to her mother abo...      0\n",
       "3        5  This is worse than a lump of coal in your stoc...      0\n",
       "4        6  Luann De Lesseps is going to rehab after her a...      0\n",
       "...    ...                                                ...    ...\n",
       "1242  1244  Get the latest from TODAY Sign up for our news...      0\n",
       "1243  1245  Jaden Smith claims that the Four Seasons Hotel...      0\n",
       "1244  1246  Overview (3)  Mini Bio (1)  Faith Hill was bor...      1\n",
       "1245  1247  CLOSE Aaron Paul dishes on 'The Path'  Aaron P...      1\n",
       "1246  1248  Meghan Edmonds was showered with love at her b...      1\n",
       "\n",
       "[1247 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把 sample_submission.csv 中的 id 與 label切開，再塞入 test\n",
    "\n",
    "label=[]\n",
    "for i in range(len(test_label[\"id,label\"])):\n",
    "    temp_label = test_label[\"id,label\"][i][-1]\n",
    "    label.append(int(temp_label))\n",
    "    \n",
    "test_label[\"label\"]=label\n",
    "\n",
    "test[\"label\"]= test_label[\"label\"]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get the latest from TODAY Sign up for our newsletter  No one ever truly gets over losing a loved one, and Blake Shelton is no exception. He was just 14 when his older brother Richie died on Nov. 13, 1990. And, as Shelton noted in a tweet Monday, \"It changed my life forever.\"  Richie was 24 when he died in a car accident in the Sheltons\\' home state of Oklahoma. Two years ago, Shelton sent out a message for the 25th anniversary of his loss:  Richie, who was Blake\\'s half-brother (they shared a mother), was a passenger in a car that collided with a school bus in Ada, south of Oklahoma City.  Richie, driver Redena McManus and a 3-year-old boy, Christopher McManus, all died during or shortly after the collision, while the bus driver and passengers were uninjured, according to police reports.  The accident has clearly remained with Blake, who told 60 Minutes in 2014, \"I remember picking up the phone to call him a week after he was dead, to tell him something. I was picking up the phone to call him, to tell him something I just saw on TV or, and it was like constantly a shock to me that he was dead.\"  Blake Shelton playing at TODAY\\'s Halloween Extravaganza in New York City on Oct. 31. Getty Images  In 2011, Blake and his then-wife Miranda Lambert wrote a single called \"Over You,\" which was inspired by Richie.  Still, the two brothers had bonded despite the age difference; both shared a love of country music. \"His bedroom was right across the hallway from mine when I was little,\" Blake said in that interview. \"And he was listening to Hank Williams, Jr. or Waylon, Lynyrd Skynyrd or Bob Seeger. I just, whatever was popular really, Richie loved all music.  \"And I would be sitting there going, \\'Man, that guy\\'s my hero. That\\'s the coolest guy. He’s my big brother.\\'\"  Follow Randee Dawn on Twitter.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4987 entries, 0 to 4986\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    4987 non-null   object\n",
      " 1   label   4987 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 78.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1247 entries, 0 to 1246\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      1247 non-null   int64 \n",
      " 1   text    1247 non-null   object\n",
      " 2   label   1247 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 29.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有個 label夾在0.1裡面，抓出來刪掉row1615\n",
    "# train[\"label\"].value_counts()\n",
    "# 0        2972\n",
    "# 1        2014\n",
    "# label       1\n",
    "# Name: label, dtype: int64\n",
    "\n",
    "\n",
    "# a=0\n",
    "# for i in train[\"label\"]:\n",
    "#     if i!=\"label\":\n",
    "#         a+=1\n",
    "#     if i==\"label\":\n",
    "#         break\n",
    "# a=1615\n",
    "train = train.drop([1615])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2972\n",
       "1    2014\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 引用 NLTK 的 stop words list\n",
    "stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# 將文字轉成次數，同時 去除停頓詞stop words \n",
    "vectorizer = CountVectorizer(stop_words=stop_words, max_features=2500)\n",
    "train_vector_count = vectorizer.fit_transform(train[\"text\"])\n",
    "test_vector_count = vectorizer.fit_transform(test[\"text\"])\n",
    "\n",
    "# 將次數轉成機率?頻率? 總之轉完才能順利跑GBDT\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transform = TfidfTransformer()\n",
    "train_vector = transform.fit_transform(train_vector_count).toarray()\n",
    "test_vector = transform.fit_transform(test_vector_count).toarray()\n",
    "\n",
    "# print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000z', '01', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1991', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '20', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '300', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '500', '51', '52', '53', '54', '55', '56', '59', '60', '61', '63', '66', '70', '72', '75', '80', '90', '99', 'abbey', 'abc', 'able', 'absolutely', 'abuse', 'academy', 'accept', 'access', 'accident', 'according', 'account', 'accounts', 'accusations', 'accused', 'across', 'act', 'acting', 'action', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actually', 'ad', 'adam', 'adaptation', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'addressed', 'adds', 'administration', 'admits', 'admitted', 'adorable', 'ads', 'adult', 'advertisement', 'advertising', 'advice', 'affair', 'affleck', 'afford', 'affordable', 'afghanistan', 'afp', 'afraid', 'african', 'afternoon', 'age', 'agency', 'agent', 'ago', 'agree', 'agreed', 'agreement', 'ahead', 'air', 'aired', 'airport', 'aisle', 'al', 'alana', 'album', 'albums', 'alcohol', 'alex', 'alexander', 'alive', 'allegations', 'alleged', 'allegedly', 'allen', 'allow', 'allowed', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'alum', 'always', 'amazing', 'america', 'american', 'americans', 'amid', 'among', 'amount', 'amy', 'andrew', 'andy', 'angeles', 'angelina', 'angie', 'aniston', 'anna', 'anne', 'anniversary', 'announced', 'announcement', 'annual', 'another', 'answer', 'anthony', 'anti', 'anybody', 'anymore', 'anyone', 'anything', 'anywhere', 'ap', 'apart', 'apartment', 'app', 'apparently', 'appeal', 'appear', 'appearance', 'appeared', 'appearing', 'appears', 'applause', 'apple', 'approach', 'april', 'area', 'ariana', 'arm', 'arms', 'army', 'around', 'arrested', 'arrive', 'arrived', 'art', 'article', 'artist', 'artists', 'arts', 'ashlee', 'ashley', 'aside', 'ask', 'asked', 'asking', 'asks', 'assault', 'assistant', 'associated', 'association', 'atlanta', 'attack', 'attacks', 'attempt', 'attend', 'attended', 'attending', 'attention', 'attorney', 'audience', 'audiences', 'aug', 'august', 'aunt', 'australia', 'australian', 'author', 'available', 'average', 'avoid', 'award', 'awards', 'aware', 'awareness', 'away', 'babies', 'baby', 'bachelor', 'bachelorette', 'back', 'background', 'bad', 'bag', 'balance', 'baldwin', 'ball', 'band', 'bank', 'bar', 'barack', 'based', 'basically', 'bass', 'batman', 'battle', 'bay', 'bbc', 'beach', 'bear', 'beat', 'beautiful', 'beauty', 'became', 'become', 'becomes', 'becoming', 'bed', 'began', 'begin', 'beginning', 'begins', 'behavior', 'behind', 'believe', 'believed', 'believes', 'bell', 'bella', 'ben', 'bendjima', 'benefit', 'best', 'better', 'beverly', 'beyoncé', 'beyond', 'bieber', 'big', 'biggest', 'bill', 'billboard', 'billion', 'birth', 'birthday', 'bit', 'black', 'blake', 'blame', 'blog', 'blood', 'blue', 'board', 'bob', 'bobby', 'body', 'bond', 'boo', 'book', 'books', 'border', 'born', 'bottom', 'bought', 'bowl', 'box', 'boy', 'boyfriend', 'boys', 'brad', 'brady', 'brand', 'break', 'breaking', 'breaks', 'breakup', 'breast', 'brexit', 'brian', 'bride', 'brief', 'briefly', 'bring', 'bringing', 'britain', 'british', 'britney', 'broadcast', 'broadway', 'broke', 'broken', 'bros', 'brother', 'brothers', 'brought', 'brown', 'bruce', 'bryan', 'bryce', 'budget', 'building', 'built', 'bump', 'bunch', 'bus', 'bush', 'business', 'busy', 'buy', 'ca', 'caitlyn', 'california', 'call', 'called', 'calling', 'calls', 'cambridge', 'came', 'camera', 'cameras', 'cameron', 'campaign', 'canada', 'canadian', 'cancer', 'candidate', 'cannot', 'capital', 'caption', 'captioned', 'car', 'card', 'cardi', 'care', 'career', 'carey', 'carl', 'carpet', 'carrey', 'carrie', 'carried', 'carry', 'cars', 'case', 'cash', 'cast', 'casting', 'castle', 'catch', 'categories', 'category', 'catherine', 'caught', 'cause', 'caused', 'cbs', 'celebrate', 'celebrated', 'celebrating', 'celebrities', 'celebrity', 'cena', 'center', 'central', 'century', 'ceremony', 'certain', 'certainly', 'chairman', 'challenge', 'chance', 'change', 'changed', 'changes', 'changing', 'channel', 'chapel', 'chapman', 'character', 'characters', 'charge', 'charges', 'charity', 'charles', 'charlie', 'charlotte', 'chart', 'cheating', 'check', 'chelsea', 'chicago', 'chief', 'child', 'children', 'china', 'choice', 'choose', 'chose', 'chris', 'christensen', 'christian', 'christina', 'christmas', 'christopher', 'church', 'chyna', 'circle', 'city', 'civil', 'claim', 'claimed', 'claiming', 'claims', 'class', 'classic', 'clay', 'clean', 'clear', 'clearly', 'cleveland', 'click', 'climate', 'clinton', 'clip', 'clooney', 'close', 'closer', 'club', 'cnn', 'co', 'coach', 'coffee', 'cohen', 'cold', 'collection', 'college', 'color', 'com', 'combined', 'come', 'comedy', 'comes', 'comfortable', 'comic', 'coming', 'comment', 'commented', 'comments', 'commercial', 'commercials', 'committed', 'committee', 'common', 'community', 'companies', 'company', 'competition', 'competitive', 'complete', 'completely', 'complicated', 'concern', 'concerned', 'concerns', 'concert', 'concluded', 'conference', 'confidence', 'confirm', 'confirmed', 'congress', 'connection', 'considered', 'constantly', 'contact', 'content', 'contestant', 'continue', 'continued', 'continues', 'continuing', 'contract', 'control', 'controversial', 'controversy', 'convention', 'conversation', 'cool', 'cooper', 'cop', 'corden', 'cost', 'costs', 'could', 'council', 'countries', 'country', 'county', 'couple', 'couples', 'course', 'court', 'courtesy', 'cousin', 'cover', 'coverage', 'covered', 'cox', 'craig', 'crash', 'crazy', 'create', 'created', 'creating', 'creative', 'credit', 'credits', 'crew', 'crime', 'criminal', 'crisis', 'critical', 'criticism', 'criticized', 'critics', 'cross', 'crowd', 'crown', 'cruise', 'crying', 'culture', 'cup', 'current', 'currently', 'custody', 'cut', 'cute', 'cyber', 'cyrus', 'dad', 'daddy', 'daenerys', 'daily', 'dance', 'dancing', 'daniel', 'danny', 'dark', 'data', 'date', 'dated', 'dating', 'daughter', 'daughters', 'dave', 'david', 'davidson', 'davis', 'day', 'days', 'dc', 'de', 'dead', 'deal', 'deals', 'death', 'debate', 'debut', 'debuted', 'dec', 'decade', 'decades', 'december', 'decided', 'decision', 'deep', 'deeply', 'defense', 'definitely', 'degeneres', 'del', 'delivered', 'democratic', 'democrats', 'denied', 'department', 'depp', 'described', 'deserve', 'design', 'designed', 'designer', 'despite', 'details', 'determined', 'development', 'diamond', 'diana', 'died', 'difference', 'differences', 'different', 'difficult', 'digital', 'dinner', 'direct', 'directed', 'direction', 'directly', 'director', 'discuss', 'disick', 'dislikes', 'disney', 'district', 'divorce', 'divorced', 'dj', 'doctor', 'documentary', 'documents', 'dog', 'dollars', 'domestic', 'donald', 'done', 'door', 'double', 'doubt', 'downhill', 'download', 'dr', 'drag', 'dragons', 'drake', 'drama', 'dream', 'dress', 'dressed', 'dresses', 'drew', 'drink', 'drive', 'driver', 'drop', 'dropped', 'drug', 'dubai', 'duchess', 'due', 'duke', 'duo', 'earlier', 'early', 'earned', 'earth', 'east', 'easy', 'eat', 'economic', 'economy', 'ed', 'edit', 'edition', 'education', 'edward', 'effect', 'effort', 'efforts', 'eight', 'either', 'el', 'election', 'eliminated', 'elizabeth', 'ellen', 'else', 'email', 'emily', 'emma', 'emmy', 'emotional', 'en', 'end', 'ended', 'ending', 'ends', 'energy', 'enforcement', 'engaged', 'engagement', 'england', 'english', 'enjoy', 'enjoyed', 'enjoying', 'enough', 'ensemble', 'ensure', 'entertainment', 'entire', 'entirely', 'entitled', 'epa', 'epic', 'episode', 'episodes', 'eric', 'especially', 'essentially', 'estate', 'estimated', 'et', 'europe', 'european', 'evan', 'evans', 'eve', 'even', 'evening', 'event', 'events', 'eventually', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'evidence', 'ex', 'exactly', 'example', 'excited', 'exclusive', 'exclusively', 'executive', 'expect', 'expected', 'expecting', 'experience', 'explained', 'explaining', 'explains', 'expressed', 'extra', 'extremely', 'eye', 'eyes', 'face', 'facebook', 'faced', 'fact', 'factor', 'facts', 'failed', 'fair', 'faith', 'fake', 'fall', 'fallon', 'false', 'fame', 'families', 'family', 'famous', 'fan', 'fans', 'fantasy', 'far', 'farm', 'fashion', 'fast', 'father', 'favorite', 'fbi', 'fear', 'feature', 'featured', 'features', 'featuring', 'february', 'federal', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'fell', 'fellow', 'felt', 'female', 'feminist', 'festival', 'feud', 'field', 'fifth', 'fight', 'fighting', 'figure', 'filed', 'filled', 'film', 'filmed', 'filming', 'films', 'final', 'finale', 'finally', 'financial', 'find', 'finds', 'fine', 'finished', 'fire', 'first', 'fisher', 'fit', 'five', 'fleming', 'flight', 'floor', 'florida', 'flowers', 'focus', 'focused', 'folks', 'follow', 'followed', 'followers', 'following', 'food', 'footage', 'football', 'force', 'forced', 'forces', 'ford', 'foreign', 'forever', 'forget', 'form', 'former', 'forth', 'forward', 'foster', 'found', 'foundation', 'founder', 'four', 'fourth', 'fox', 'foxx', 'france', 'franchise', 'frank', 'free', 'french', 'fresh', 'friday', 'friend', 'friends', 'friendship', 'front', 'full', 'fully', 'fun', 'fund', 'funeral', 'funny', 'future', 'gadot', 'gaga', 'gala', 'gallery', 'game', 'games', 'garner', 'gary', 'gave', 'gay', 'gender', 'general', 'generally', 'generation', 'george', 'georgia', 'get', 'gets', 'getting', 'getty', 'giant', 'gift', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'glad', 'global', 'globe', 'go', 'goal', 'god', 'goes', 'going', 'gold', 'golden', 'golf', 'gomez', 'gone', 'gonna', 'good', 'goodbye', 'gore', 'gorgeous', 'gossip', 'got', 'gotten', 'government', 'governor', 'gown', 'grace', 'grammy', 'grammys', 'grande', 'grateful', 'great', 'greatest', 'green', 'greenfield', 'grew', 'grey', 'groom', 'ground', 'group', 'grow', 'growing', 'grown', 'guess', 'guest', 'guests', 'guilty', 'gun', 'guns', 'guy', 'guys', 'gwen', 'haddish', 'hadid', 'hailey', 'hair', 'half', 'hall', 'halloween', 'hand', 'hands', 'hanging', 'hannah', 'happen', 'happened', 'happening', 'happens', 'happy', 'harassment', 'hard', 'harper', 'harris', 'harrison', 'harry', 'harvey', 'hate', 'hbo', 'head', 'headed', 'headlines', 'health', 'healthy', 'hear', 'heard', 'hearing', 'heart', 'heidi', 'held', 'hell', 'help', 'helped', 'helping', 'hemsworth', 'henry', 'hernandez', 'hey', 'hide', 'high', 'highest', 'highly', 'hill', 'hillary', 'hills', 'hillsong', 'hip', 'history', 'hit', 'hits', 'hold', 'holding', 'holiday', 'hollywood', 'hollywoodlife', 'holmes', 'holt', 'home', 'homes', 'honest', 'honestly', 'honey', 'honor', 'honored', 'hop', 'hope', 'hopes', 'hoping', 'hospital', 'host', 'hosted', 'hot', 'hotel', 'hour', 'hours', 'house', 'housewives', 'houston', 'howard', 'however', 'http', 'https', 'huge', 'hulu', 'human', 'humanitarian', 'humor', 'hundreds', 'hurt', 'husband', 'ice', 'iconic', 'idea', 'ideas', 'idol', 'iheartradio', 'ii', 'image', 'images', 'imagine', 'immediately', 'immigration', 'impact', 'important', 'inbox', 'incident', 'include', 'included', 'includes', 'including', 'income', 'incredible', 'incredibly', 'independent', 'individual', 'industry', 'influence', 'information', 'initially', 'injured', 'inside', 'insider', 'inspired', 'instagram', 'instead', 'insurance', 'intelligence', 'interest', 'interested', 'interesting', 'interests', 'international', 'internet', 'interview', 'intimate', 'introduced', 'investigation', 'invited', 'involved', 'iphone', 'iran', 'iraq', 'ireland', 'island', 'israel', 'issue', 'issues', 'italy', 'items', 'ivanka', 'jack', 'jackson', 'james', 'jamie', 'jane', 'january', 'jason', 'jay', 'jeff', 'jen', 'jenkins', 'jenna', 'jenner', 'jennifer', 'jersey', 'jessica', 'jesus', 'jim', 'jimmy', 'joan', 'job', 'jobs', 'joe', 'john', 'johnny', 'johnson', 'join', 'joined', 'joining', 'joint', 'joke', 'joked', 'jolie', 'jon', 'jones', 'jordan', 'jordyn', 'josh', 'journalist', 'journey', 'joy', 'jr', 'judd', 'judge', 'judges', 'judy', 'julia', 'july', 'june', 'justice', 'justin', 'kanye', 'kardashian', 'kardashians', 'kate', 'katie', 'katy', 'keep', 'keeping', 'kelly', 'kendall', 'kendrick', 'kept', 'kevin', 'key', 'khloe', 'khloé', 'kid', 'kidman', 'kids', 'kill', 'killed', 'kim', 'kimmel', 'kind', 'kinds', 'king', 'kingdom', 'kiss', 'kissing', 'knew', 'knot', 'know', 'knowing', 'known', 'knows', 'kotb', 'kourtney', 'kris', 'kristen', 'kyle', 'kylie', 'la', 'label', 'labor', 'lace', 'lack', 'lady', 'lamar', 'lambert', 'land', 'language', 'large', 'largely', 'las', 'last', 'late', 'later', 'latest', 'latin', 'lauer', 'laugh', 'laughing', 'laughs', 'laughter', 'launch', 'launched', 'laura', 'lauren', 'law', 'lawrence', 'lawsuit', 'lawyer', 'lawyers', 'lead', 'leader', 'leaders', 'leading', 'leads', 'league', 'learn', 'learned', 'learning', 'least', 'leave', 'leaves', 'leaving', 'led', 'ledger', 'lee', 'left', 'legal', 'length', 'less', 'let', 'letter', 'level', 'lewis', 'liam', 'lies', 'life', 'lifestyle', 'lifetime', 'light', 'like', 'liked', 'likely', 'likes', 'limited', 'lindsay', 'line', 'lines', 'link', 'lip', 'list', 'listen', 'listening', 'literally', 'little', 'live', 'lived', 'lives', 'living', 'local', 'location', 'lohan', 'london', 'long', 'longer', 'longtime', 'look', 'looked', 'looking', 'looks', 'lopez', 'los', 'lose', 'losing', 'loss', 'lost', 'lot', 'lots', 'louis', 'love', 'loved', 'lovely', 'loves', 'loving', 'low', 'lower', 'lucky', 'luke', 'lunch', 'ly', 'lyrics', 'mad', 'maddox', 'made', 'madonna', 'magazine', 'mail', 'main', 'major', 'make', 'makes', 'makeup', 'making', 'male', 'malibu', 'mama', 'man', 'management', 'manager', 'mandy', 'many', 'marc', 'march', 'marchesa', 'mariah', 'mark', 'market', 'markle', 'marriage', 'married', 'marry', 'mars', 'martin', 'martinez', 'marvel', 'mary', 'mason', 'massive', 'master', 'match', 'material', 'matt', 'matter', 'matters', 'matthew', 'matthews', 'may', 'maybe', 'mccain', 'mean', 'means', 'meant', 'meanwhile', 'medal', 'media', 'medical', 'meet', 'meeting', 'megan', 'meghan', 'megyn', 'melissa', 'member', 'members', 'memoir', 'men', 'mental', 'message', 'met', 'mexican', 'mexico', 'miami', 'michael', 'michelle', 'mid', 'middle', 'middleton', 'might', 'mike', 'miley', 'military', 'miller', 'million', 'millions', 'mind', 'mine', 'minister', 'minute', 'minutes', 'miranda', 'misconduct', 'miss', 'missed', 'missing', 'mix', 'mixed', 'model', 'modern', 'mogul', 'mom', 'moment', 'moments', 'monday', 'money', 'monica', 'month', 'months', 'moore', 'morgan', 'morning', 'mostly', 'mother', 'motion', 'move', 'moved', 'movement', 'movie', 'movies', 'moving', 'mr', 'mrs', 'ms', 'mtv', 'much', 'multiple', 'murder', 'murray', 'music', 'musical', 'musician', 'must', 'mutual', 'nafta', 'name', 'named', 'names', 'nashville', 'nation', 'national', 'natural', 'nature', 'nbc', 'near', 'nearly', 'need', 'needed', 'needs', 'negative', 'neither', 'net', 'netflix', 'network', 'never', 'new', 'news', 'newsletter', 'next', 'nfl', 'nice', 'nick', 'nicole', 'night', 'nine', 'nobody', 'nominated', 'nomination', 'nominations', 'nominee', 'non', 'none', 'normal', 'north', 'note', 'noted', 'notes', 'nothing', 'notice', 'notifications', 'noting', 'nov', 'novel', 'november', 'nuclear', 'number', 'numerous', 'nuptials', 'obama', 'obtained', 'obviously', 'ocean', 'october', 'offer', 'offered', 'office', 'official', 'officially', 'officials', 'often', 'oh', 'ohio', 'ok', 'okay', 'old', 'older', 'olympic', 'olympics', 'one', 'ones', 'ongoing', 'online', 'onto', 'open', 'opened', 'opening', 'opens', 'opportunity', 'opposite', 'order', 'organization', 'original', 'originally', 'oscar', 'oscars', 'others', 'outfit', 'outlet', 'outside', 'outstanding', 'overall', 'owner', 'pa', 'page', 'paid', 'pain', 'pair', 'palace', 'paparazzi', 'paradise', 'parent', 'parents', 'paris', 'park', 'parker', 'parliament', 'part', 'particular', 'particularly', 'parties', 'partner', 'partners', 'parts', 'party', 'pass', 'passed', 'passion', 'past', 'patrick', 'pattinson', 'paul', 'paxton', 'pay', 'paying', 'pdt', 'peace', 'penn', 'people', 'per', 'percent', 'perfect', 'perform', 'performance', 'performances', 'performed', 'performing', 'perhaps', 'period', 'perry', 'person', 'personal', 'personality', 'pete', 'peter', 'philip', 'phone', 'photo', 'photographed', 'photographs', 'photos', 'physical', 'pic', 'pick', 'picked', 'pics', 'picture', 'pictured', 'pictures', 'piece', 'pieces', 'pilot', 'pink', 'pinterest', 'pippa', 'pitt', 'place', 'placed', 'places', 'plan', 'plane', 'planet', 'planned', 'planning', 'plans', 'plastic', 'platform', 'platt', 'play', 'played', 'player', 'playing', 'plays', 'please', 'plenty', 'plot', 'plus', 'point', 'points', 'police', 'policy', 'political', 'politics', 'poor', 'pop', 'popular', 'portrayal', 'portrayed', 'position', 'positive', 'possible', 'possibly', 'post', 'posted', 'posting', 'posts', 'potential', 'power', 'powerful', 'praised', 'pre', 'pregnancy', 'pregnant', 'premiere', 'premiered', 'prepared', 'present', 'presented', 'president', 'presidential', 'press', 'pressure', 'pretty', 'previous', 'previously', 'price', 'primary', 'prime', 'prince', 'princess', 'prior', 'prison', 'privacy', 'private', 'pro', 'probably', 'problem', 'problems', 'process', 'produced', 'producer', 'producers', 'production', 'productions', 'products', 'professional', 'profile', 'program', 'project', 'projects', 'promote', 'promoting', 'property', 'proposal', 'proposed', 'protect', 'protest', 'proud', 'provide', 'provided', 'pst', 'public', 'publication', 'publicly', 'published', 'pulled', 'pumpkin', 'purchase', 'push', 'pushed', 'put', 'putin', 'putting', 'quality', 'queen', 'question', 'questions', 'quick', 'quickly', 'quite', 'race', 'rachel', 'radar', 'radio', 'raise', 'raised', 'raising', 'ran', 'rap', 'rape', 'rapper', 'rate', 'rather', 'rating', 'ratings', 'ray', 'reach', 'reached', 'reaction', 'read', 'reading', 'ready', 'real', 'realdonaldtrump', 'reality', 'realize', 'realized', 'really', 'reason', 'reasons', 'reboot', 'recalled', 'receive', 'received', 'receiving', 'recent', 'recently', 'reception', 'record', 'recording', 'records', 'red', 'reed', 'reference', 'refused', 'regarding', 'regular', 'related', 'relationship', 'relationships', 'release', 'released', 'religious', 'remain', 'remained', 'remains', 'remember', 'removed', 'rep', 'repeatedly', 'reply', 'report', 'reported', 'reportedly', 'reporter', 'reporters', 'reports', 'representative', 'republican', 'republicans', 'reputation', 'request', 'research', 'respect', 'respond', 'responded', 'response', 'responsibility', 'responsible', 'rest', 'restaurant', 'result', 'results', 'return', 'returned', 'returning', 'returns', 'reunion', 'reuters', 'reveal', 'revealed', 'revealing', 'reveals', 'review', 'reviews', 'reynolds', 'rich', 'richard', 'richie', 'ride', 'right', 'rights', 'rihanna', 'ring', 'risk', 'road', 'rob', 'robert', 'roberts', 'rock', 'rodriguez', 'roger', 'role', 'roles', 'roll', 'rolling', 'romance', 'romantic', 'room', 'rose', 'roseanne', 'ross', 'round', 'royal', 'rumored', 'rumors', 'run', 'running', 'rupaul', 'russert', 'russia', 'russian', 'ryan', 'sad', 'safe', 'said', 'sales', 'sam', 'samantha', 'san', 'sanders', 'santa', 'sara', 'sarah', 'sat', 'saturday', 'saudi', 'save', 'saw', 'say', 'saying', 'says', 'scandal', 'scene', 'scenes', 'schedule', 'scheduled', 'school', 'schools', 'schumer', 'scientology', 'score', 'scott', 'screen', 'script', 'seacrest', 'sean', 'search', 'season', 'seasons', 'second', 'secret', 'secretary', 'secrets', 'security', 'see', 'seeing', 'seeking', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'segment', 'selena', 'self', 'sen', 'senate', 'senator', 'senators', 'senior', 'sense', 'sent', 'separate', 'separated', 'separation', 'sept', 'september', 'sequel', 'series', 'serious', 'seriously', 'serve', 'served', 'service', 'services', 'serving', 'set', 'setting', 'settlement', 'seven', 'several', 'sex', 'sexual', 'shades', 'shannon', 'shape', 'share', 'shared', 'shares', 'sharing', 'shawn', 'sheeran', 'shelton', 'sheriff', 'shirt', 'shocked', 'shocking', 'shoes', 'shookus', 'shoot', 'shooting', 'short', 'shortly', 'shot', 'show', 'showed', 'showing', 'shown', 'shows', 'sick', 'side', 'sign', 'signed', 'significant', 'silver', 'similar', 'simple', 'simply', 'simpson', 'since', 'sing', 'singer', 'singing', 'single', 'sir', 'sister', 'sisters', 'sit', 'site', 'sitting', 'situation', 'six', 'sixth', 'ski', 'skin', 'slalom', 'small', 'smart', 'smile', 'smith', 'snapchat', 'social', 'society', 'sofia', 'sold', 'solo', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'son', 'song', 'songs', 'songwriter', 'sons', 'sony', 'soon', 'sorry', 'sort', 'soul', 'sound', 'sounds', 'source', 'sources', 'south', 'space', 'speak', 'speaking', 'spears', 'special', 'specific', 'specifically', 'speculation', 'speech', 'spencer', 'spend', 'spending', 'spent', 'spirit', 'splash', 'split', 'splits', 'spoke', 'sports', 'spot', 'spotted', 'spread', 'spring', 'st', 'staff', 'stage', 'stand', 'standard', 'standards', 'standing', 'stands', 'star', 'starred', 'starring', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'statements', 'states', 'stating', 'station', 'status', 'stay', 'stayed', 'stefani', 'step', 'stephanopoulos', 'stephen', 'stern', 'steve', 'stewart', 'still', 'stone', 'stood', 'stop', 'stopped', 'store', 'stories', 'story', 'straight', 'stranger', 'streaming', 'streep', 'street', 'strong', 'stronger', 'student', 'students', 'studio', 'stuff', 'stunning', 'style', 'styles', 'subject', 'subscribe', 'subsequently', 'success', 'successful', 'suffered', 'suggested', 'suicide', 'suit', 'suits', 'summer', 'sun', 'sunday', 'super', 'superhero', 'support', 'supported', 'supporting', 'supposed', 'sure', 'surgery', 'surprise', 'surprised', 'surrounding', 'sussex', 'sweet', 'swift', 'system', 'table', 'tabloid', 'take', 'taken', 'takes', 'taking', 'tale', 'talent', 'talk', 'talked', 'talking', 'talks', 'tape', 'tapes', 'target', 'tax', 'taxes', 'taylor', 'team', 'tears', 'teen', 'television', 'tell', 'telling', 'tells', 'ten', 'tennis', 'term', 'terms', 'terrible', 'test', 'texas', 'thank', 'thanks', 'theater', 'theaters', 'theatre', 'theme', 'theresa', 'theroux', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'thomas', 'thompson', 'though', 'thought', 'thoughts', 'thousands', 'thread', 'three', 'throughout', 'thursday', 'ticket', 'tie', 'tied', 'tim', 'time', 'times', 'tiny', 'title', 'titled', 'tmz', 'today', 'together', 'told', 'tom', 'tomorrow', 'tonight', 'tony', 'took', 'top', 'toronto', 'total', 'totally', 'touch', 'tough', 'tour', 'toward', 'towards', 'town', 'track', 'trade', 'trailer', 'training', 'transgender', 'travel', 'travis', 'treatment', 'trial', 'tribute', 'tried', 'trip', 'tristan', 'trouble', 'true', 'truly', 'trump', 'trust', 'truth', 'try', 'trying', 'tuesday', 'turn', 'turned', 'turns', 'tv', 'tweet', 'tweeted', 'tweets', 'twice', 'twilight', 'twins', 'twitter', 'two', 'tyler', 'type', 'uk', 'ultimately', 'uncle', 'understand', 'understanding', 'unfortunately', 'union', 'unique', 'united', 'universal', 'universe', 'university', 'unknown', 'upcoming', 'update', 'updates', 'upon', 'upset', 'us', 'usa', 'use', 'used', 'using', 'usually', 'vacation', 'valentine', 'vanity', 'variety', 'various', 'vegas', 'version', 'via', 'vice', 'victim', 'victims', 'victoria', 'victory', 'video', 'videos', 'view', 'viewers', 'views', 'violence', 'visit', 'visited', 'vocal', 'voice', 'vote', 'voted', 'voters', 'votes', 'wait', 'waiting', 'wake', 'wales', 'walk', 'walked', 'walking', 'wall', 'want', 'wanted', 'wanting', 'wants', 'war', 'warm', 'warner', 'wars', 'washington', 'watch', 'watched', 'watching', 'water', 'wave', 'way', 'ways', 'wear', 'wearing', 'website', 'wed', 'wedding', 'wednesday', 'week', 'weekend', 'weekly', 'weeknd', 'weeks', 'weight', 'weinstein', 'weisz', 'welcome', 'welcomed', 'well', 'went', 'west', 'westworld', 'whatever', 'whether', 'white', 'whole', 'whose', 'wide', 'wife', 'wild', 'william', 'williams', 'willing', 'win', 'window', 'windsor', 'wine', 'winner', 'winners', 'winning', 'winter', 'wireimage', 'wish', 'within', 'without', 'woman', 'women', 'wonder', 'wonderful', 'woodruff', 'woods', 'word', 'words', 'wore', 'work', 'worked', 'workers', 'working', 'works', 'world', 'worldwide', 'worried', 'worse', 'worst', 'worth', 'would', 'write', 'writer', 'writing', 'written', 'wrong', 'wrote', 'www', 'yeah', 'year', 'years', 'yellow', 'yes', 'yesterday', 'yet', 'york', 'younes', 'young', 'younger', 'youngest', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 59)\t1\n",
      "  (0, 89)\t1\n",
      "  (0, 119)\t2\n",
      "  (0, 120)\t1\n",
      "  (0, 125)\t1\n",
      "  (0, 162)\t1\n",
      "  (0, 165)\t1\n",
      "  (0, 216)\t1\n",
      "  (0, 342)\t1\n",
      "  (0, 353)\t6\n",
      "  (0, 359)\t1\n",
      "  (0, 370)\t1\n",
      "  (0, 395)\t3\n",
      "  (0, 396)\t1\n",
      "  (0, 413)\t2\n",
      "  (0, 414)\t1\n",
      "  (0, 431)\t2\n",
      "  (0, 468)\t1\n",
      "  (0, 501)\t1\n",
      "  (0, 504)\t2\n",
      "  :\t:\n",
      "  (0, 2038)\t1\n",
      "  (0, 2048)\t1\n",
      "  (0, 2062)\t1\n",
      "  (0, 2069)\t1\n",
      "  (0, 2090)\t2\n",
      "  (0, 2105)\t1\n",
      "  (0, 2148)\t1\n",
      "  (0, 2164)\t1\n",
      "  (0, 2242)\t2\n",
      "  (0, 2288)\t2\n",
      "  (0, 2290)\t1\n",
      "  (0, 2325)\t1\n",
      "  (0, 2336)\t1\n",
      "  (0, 2337)\t1\n",
      "  (0, 2343)\t1\n",
      "  (0, 2344)\t2\n",
      "  (0, 2425)\t1\n",
      "  (0, 2438)\t1\n",
      "  (0, 2444)\t1\n",
      "  (0, 2447)\t1\n",
      "  (0, 2481)\t1\n",
      "  (0, 2487)\t1\n",
      "  (0, 2490)\t1\n",
      "  (0, 2491)\t1\n",
      "  (0, 2495)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_vector_count[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train & test data for xgboost\n",
    "train_X = train_vector\n",
    "train_y = train[\"label\"].values\n",
    "test_X = test_vector\n",
    "test_y = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGboost precision= 0.5138504155124654\n",
      "XGboost recall= 0.6012965964343598\n",
      "XGboost accuracy= 0.5212510024057738\n",
      "XGboost f1_score= 0.5541448842419716\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "xgboost\n",
    "\"\"\"\n",
    "import xgboost as xgb\n",
    "\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix(train_X , label=train_y)\n",
    "dtest = xgb.DMatrix(test_X , label=test_y)\n",
    "# specify parameters via map\n",
    "param = {'max_depth': 100, 'eta': 1, 'objective': 'reg:logistic'}\n",
    "param['nthread'] = 4\n",
    "#param['eval_metric'] = 'auc'\n",
    "\n",
    "num_round = 4 # 越靠近3或4，模型評分越好。\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "#After training, the model can be saved.\n",
    "bst.save_model('0001.model')\n",
    "\n",
    "#A saved model can be loaded as follows:\n",
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model('0001.model')  # load data\n",
    "\n",
    "# make prediction\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "\n",
    "# 計算Precision, Recall, Accuracy\n",
    "# II. 函式計算\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "precision = precision_score(test_y,y_pred.round())\n",
    "recall = recall_score(test_y,y_pred.round())\n",
    "accuracy = accuracy_score(test_y,y_pred.round())\n",
    "f1_measure = f1_score(test_y,y_pred.round())\n",
    "print(\"XGboost precision=\",precision)\n",
    "print(\"XGboost recall=\",recall)\n",
    "print(\"XGboost accuracy=\",accuracy)\n",
    "print(\"XGboost f1_score=\",f1_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#畫出 XGB的樹\n",
    "xgb.to_graphviz(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using only CountVectorizer, not using TfidfTransformer:\n",
    "# XGboost precision= 0.47985347985347987\n",
    "# XGboost recall= 0.21231766612641814\n",
    "# XGboost accuracy= 0.4963913392141139\n",
    "# XGboost f1_score= 0.2943820224719101\n",
    "\n",
    "# param = {'max_depth': 25, 'eta': 1, 'objective': 'reg:logistic'}, param['nthread'] = 4, num_round = 2\n",
    "# XGboost precision= 0.5114942528735632\n",
    "# XGboost recall= 0.5769854132901134\n",
    "# XGboost accuracy= 0.5180433039294307\n",
    "# XGboost f1_score= 0.5422696115765423\n",
    "\n",
    "# 'max_depth': 25→10\n",
    "# XGboost precision= 0.5278969957081545\n",
    "# XGboost recall= 0.3987034035656402\n",
    "# XGboost accuracy= 0.5260625501202887\n",
    "# XGboost f1_score= 0.45429362880886426\n",
    "\n",
    "# 'max_depth': 25→100\n",
    "# XGboost precision= 0.5080645161290323\n",
    "# XGboost recall= 0.6126418152350082\n",
    "# XGboost accuracy= 0.5148356054530874\n",
    "# XGboost f1_score= 0.5554739162380602\n",
    "\n",
    "# 'max_depth': 25→200→可見100已達飽和點\n",
    "# XGboost precision= 0.5080645161290323\n",
    "# XGboost recall= 0.6126418152350082\n",
    "# XGboost accuracy= 0.5148356054530874\n",
    "# XGboost f1_score= 0.5554739162380602\n",
    "\n",
    "# 確定將'max_depth'設定為100之後：\n",
    "# 'eta':1→3，評分全面下降，確定將 'eta'設為1\n",
    "# XGboost precision= 0.4692005242463958\n",
    "# XGboost recall= 0.580226904376013\n",
    "# XGboost accuracy= 0.46752205292702487\n",
    "# XGboost f1_score= 0.518840579710145\n",
    "\n",
    "# param['nthread'] = 4→10：評分完全沒變，所以維持用4\n",
    "# XGboost precision= 0.5080645161290323\n",
    "# XGboost recall= 0.6126418152350082\n",
    "# XGboost accuracy= 0.5148356054530874\n",
    "# XGboost f1_score= 0.5554739162380602\n",
    "\n",
    "# num_round = 2→6\n",
    "# XGboost precision= 0.4978038067349927\n",
    "# XGboost recall= 0.5510534846029174\n",
    "# XGboost accuracy= 0.5028067361668003\n",
    "# XGboost f1_score= 0.523076923076923\n",
    "\n",
    "# num_round = 2→5\n",
    "# XGboost precision= 0.502906976744186\n",
    "# XGboost recall= 0.5607779578606159\n",
    "# XGboost accuracy= 0.5084202085004009\n",
    "# XGboost f1_score= 0.5302681992337166\n",
    "\n",
    "# num_round = 2→4\n",
    "# XGboost precision= 0.5138504155124654\n",
    "# XGboost recall= 0.6012965964343598\n",
    "# XGboost accuracy= 0.5212510024057738\n",
    "# XGboost f1_score= 0.5541448842419716\n",
    "\n",
    "# num_round = 2→3\n",
    "# XGboost precision= 0.5026809651474531\n",
    "# XGboost recall= 0.6077795786061588\n",
    "# XGboost accuracy= 0.5084202085004009\n",
    "# XGboost f1_score= 0.5502567865003669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT need y to be int.\n",
    "train_y=[]\n",
    "for i in train[\"label\"]:\n",
    "    a = int(i)\n",
    "    train_y.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train & test data for GBDT\n",
    "train_X = train_vector\n",
    "train_y = train_y\n",
    "test_X = test_vector\n",
    "test_y = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT precision= 0.49617672047578587\n",
      "GBDT recall= 0.946515397082658\n",
      "GBDT accuracy= 0.49799518845228546\n",
      "GBDT f1_score= 0.6510590858416945\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GBDT\n",
    "\"\"\"\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(learning_rate=5.0, max_depth=5, random_state=43)\n",
    "clf.fit(train_X, train_y)\n",
    "y_pred_GBDT=clf.predict(test_X)\n",
    "\n",
    "# 計算Precision, Recall, Accuracy\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "precision = precision_score(test_y,y_pred_GBDT.round())\n",
    "recall = recall_score(test_y,y_pred_GBDT.round())\n",
    "accuracy = accuracy_score(test_y,y_pred_GBDT.round())\n",
    "f1_measure = f1_score(test_y,y_pred_GBDT.round())\n",
    "print(\"GBDT precision=\",precision)\n",
    "print(\"GBDT recall=\",recall)\n",
    "print(\"GBDT accuracy=\",accuracy)\n",
    "print(\"GBDT f1_score=\",f1_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate=1.0, max_depth=1, random_state=0, n_estimators=100, subsample=1.0\n",
    "# GBDT precision= 0.5386416861826698\n",
    "# GBDT recall= 0.3727714748784441\n",
    "# GBDT accuracy= 0.5316760224538893\n",
    "# GBDT f1_score= 0.4406130268199234\n",
    "\n",
    "# subsample 1.0→0.5 ：沒有很大幫助\n",
    "# GBDT precision= 0.49714285714285716\n",
    "# GBDT recall= 0.42301458670988656\n",
    "# GBDT accuracy= 0.5028067361668003\n",
    "# GBDT f1_score= 0.45709281961471104\n",
    "\n",
    "# learning_rate=5.0, max_depth=5, random_state=0, n_estimators=100\n",
    "# GBDT precision= 0.4953350296861747\n",
    "# GBDT recall= 0.946515397082658\n",
    "# GBDT accuracy= 0.4963913392141139\n",
    "# GBDT f1_score= 0.6503340757238306\n",
    "\n",
    "# random_state=0→43，為什麼random_state=0 → 43； precision和accuracy會上升?\n",
    "# GBDT precision= 0.49617672047578587\n",
    "# GBDT recall= 0.946515397082658\n",
    "# GBDT accuracy= 0.49799518845228546\n",
    "# GBDT f1_score= 0.6510590858416945\n",
    "\n",
    "# learning_rate=1.0, max_depth=5, random_state=43, n_estimators=500， n_estimators放大五倍，不如learning_rate放大五倍\n",
    "# GBDT precision= 0.4888888888888889\n",
    "# GBDT recall= 0.39222042139384117\n",
    "# GBDT accuracy= 0.4963913392141139\n",
    "# GBDT f1_score= 0.43525179856115104\n",
    "\n",
    "# learning_rate=1.0, max_depth=10, random_state=0, n_estimators=100\n",
    "# GBDT precision= 0.5284090909090909\n",
    "# GBDT recall= 0.4521880064829822\n",
    "# GBDT accuracy= 0.529270248596632\n",
    "# GBDT f1_score= 0.4873362445414848\n",
    "\n",
    "# learning_rate=5.0, max_depth=10, random_state=0, n_estimators=100\n",
    "# GBDT precision= 0.4831981460023175\n",
    "# GBDT recall= 0.6758508914100486\n",
    "# GBDT accuracy= 0.48195669607056935\n",
    "# GBDT f1_score= 0.5635135135135135\n",
    "\n",
    "# learning_rate=5.0, max_depth=6, random_state=0, n_estimators=100\n",
    "# GBDT precision= 0.4919695688926458\n",
    "# GBDT recall= 0.9432739059967585\n",
    "# GBDT accuracy= 0.48997594226142743\n",
    "# GBDT f1_score= 0.6466666666666667\n",
    "\n",
    "# learning_rate=3.0, max_depth=6, random_state=0, n_estimators=100\n",
    "# GBDT precision= 0.5348189415041783\n",
    "# GBDT recall= 0.31118314424635335\n",
    "# GBDT accuracy= 0.5252606255012029\n",
    "# GBDT f1_score= 0.39344262295081966\n",
    "\n",
    "# learning_rate=5.0, max_depth=4, random_state=0, n_estimators=100\n",
    "# GBDT precision= 0.4941275167785235\n",
    "# GBDT recall= 0.9546191247974068\n",
    "# GBDT accuracy= 0.49398556535685645\n",
    "# GBDT f1_score= 0.6511885019347705\n",
    "\n",
    "# learning_rate=5.0, max_depth=4, random_state=0, n_estimators=500：learning_rate=0 讓每顆小樹權重備縮小，所以500顆樹跟100顆樹表現一樣\n",
    "# GBDT precision= 0.4941275167785235\n",
    "# GBDT recall= 0.9546191247974068\n",
    "# GBDT accuracy= 0.49398556535685645\n",
    "# GBDT f1_score= 0.6511885019347705\n",
    "\n",
    "# learning_rate=5→4.0\n",
    "# GBDT precision= 0.4792099792099792\n",
    "# GBDT recall= 0.747163695299838\n",
    "# GBDT accuracy= 0.4731355252606255\n",
    "# GBDT f1_score= 0.5839138695376821\n",
    "\n",
    "# learning_rate=5→6\n",
    "# GBDT precision= 0.4804772234273319\n",
    "# GBDT recall= 0.7179902755267423\n",
    "# GBDT accuracy= 0.47634322373696875\n",
    "# GBDT f1_score= 0.575698505523067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train & test data for LightGBM\n",
    "train_X = train_vector\n",
    "train_y = train[\"label\"].values\n",
    "test_X = test_vector\n",
    "test_y = test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2014, number of negative: 2972\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 209339\n",
      "[LightGBM] [Info] Number of data points in the train set: 4986, number of used features: 2482\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.403931 -> initscore=-0.389112\n",
      "[LightGBM] [Info] Start training from score -0.389112\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Light GBM\n",
    "\"\"\"\n",
    "\n",
    "import lightgbm as lgb \n",
    "\n",
    "train_data = lgb.Dataset(train_X , label=train_y)\n",
    "\n",
    "param = {'num_leaves': 50, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "num_round = 100\n",
    "bst = lgb.train(param, train_data, num_round)\n",
    "\n",
    "# After training, the model can be saved:\n",
    "bst.save_model('model.txt')\n",
    "\n",
    "# A saved model can be loaded:\n",
    "bst = lgb.Booster(model_file='model.txt')  # init model\n",
    "\n",
    "ypred_LightGBM = bst.predict(test_X, predict_disable_shape_check=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM precision= 0.5184466019417475\n",
      "Light GBM recall= 0.4327390599675851\n",
      "Light GBM accuracy= 0.520449077786688\n",
      "Light GBM f1_score= 0.4717314487632509\n"
     ]
    }
   ],
   "source": [
    "# 計算Precision, Recall, Accuracy\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "precision_LightGBM = precision_score(test_y,ypred_LightGBM.round())\n",
    "recall_LightGBM = recall_score(test_y,ypred_LightGBM.round())\n",
    "accuracy_LightGBM = accuracy_score(test_y,ypred_LightGBM.round())\n",
    "f1_measure_LightGBM = f1_score(test_y,ypred_LightGBM.round())\n",
    "print(\"Light GBM precision=\",precision_LightGBM)\n",
    "print(\"Light GBM recall=\",recall_LightGBM)\n",
    "print(\"Light GBM accuracy=\",accuracy_LightGBM)\n",
    "print(\"Light GBM f1_score=\",f1_measure_LightGBM)\n",
    "# 加入Tfidftransform之前\n",
    "# Light GBM precision= 0.49390243902439024\n",
    "# Light GBM recall= 0.1312803889789303\n",
    "# Light GBM accuracy= 0.5036086607858862\n",
    "# Light GBM f1_score= 0.20742637644046094\n",
    "\n",
    "# param = {'num_leaves': 31,\n",
    "#num_round = 10\n",
    "# Light GBM precision= 0.5384615384615384\n",
    "# Light GBM recall= 0.23824959481361427\n",
    "# Light GBM accuracy= 0.5220529270248596\n",
    "# Light GBM f1_score= 0.33033707865168543\n",
    "\n",
    "# param = {'num_leaves': 60,\n",
    "#num_round = 10\n",
    "# Light GBM precision= 0.5047619047619047\n",
    "# Light GBM recall= 0.2576985413290113\n",
    "# Light GBM accuracy= 0.5076182838813151\n",
    "# Light GBM f1_score= 0.3412017167381974\n",
    "\n",
    "# param = {'num_leaves': 50, \n",
    "# num_round = 10\n",
    "# Light GBM precision= 0.5317220543806647\n",
    "# Light GBM recall= 0.2852512155591572\n",
    "# Light GBM accuracy= 0.5220529270248596\n",
    "# Light GBM f1_score= 0.37130801687763715\n",
    "\n",
    "# param = {'num_leaves': 50, \n",
    "# num_round = 50\n",
    "# Light GBM precision= 0.5097276264591439\n",
    "# Light GBM recall= 0.4246353322528363\n",
    "# Light GBM accuracy= 0.5132317562149158\n",
    "# Light GBM f1_score= 0.46330680813439434\n",
    "\n",
    "# param = {'num_leaves': 50, \n",
    "# num_round = 100\n",
    "# Light GBM precision= 0.5184466019417475\n",
    "# Light GBM recall= 0.4327390599675851\n",
    "# Light GBM accuracy= 0.520449077786688\n",
    "# Light GBM f1_score= 0.4717314487632509\n",
    "\n",
    "\n",
    "# Light GBM precision= 0.5184466019417475\n",
    "# Light GBM recall= 0.4327390599675851\n",
    "# Light GBM accuracy= 0.520449077786688\n",
    "# Light GBM f1_score= 0.4717314487632509\n",
    "# CounterVectorize有沒有加 tfidftransformer，模型精度是一樣的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
